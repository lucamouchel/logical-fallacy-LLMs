{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "AUGMENT_WITH_NEUTRAL_ARGS = True\n",
    "data_dir = \"data/argumentation\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train_iam.tsv'), sep='\\t')\n",
    "dev_df = pd.read_csv(os.path.join(data_dir, 'dev_iam.txt'), sep='\\t')\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test_iam.txt'), sep='\\t')\n",
    "all_claims = pd.read_csv(os.path.join(data_dir, 'claims.txt'), sep='\\t')\n",
    "np.random.seed(42)\n",
    "\n",
    "if AUGMENT_WITH_NEUTRAL_ARGS:\n",
    "    neutral_claims = all_claims[all_claims.type=='O'] \n",
    "    lower_bound = 0\n",
    "    \n",
    "    min_train_label = min(train_df['label'].value_counts())\n",
    "    train_sample = neutral_claims.iloc[:min_train_label]\n",
    "    train_df = pd.concat([train_df, train_sample]).sample(frac=1)\n",
    "    lower_bound = min_train_label\n",
    "    \n",
    "    min_dev_label = min(dev_df['label'].value_counts())\n",
    "    dev_sample = neutral_claims.iloc[lower_bound: lower_bound + min_dev_label]    \n",
    "    dev_df = pd.concat([dev_df, dev_sample]).sample(frac=1)\n",
    "    lower_bound = lower_bound + min_dev_label\n",
    "    \n",
    "    min_test_label = min(test_df['label'].value_counts())\n",
    "    test_sample = neutral_claims.iloc[lower_bound: lower_bound + min_test_label]    \n",
    "    test_df = pd.concat([test_df, test_sample]).sample(frac=1)\n",
    "    \n",
    "    \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['label'])\n",
    "train_df['label'] = label_encoder.transform(train_df['label'])\n",
    "dev_df['label'] = label_encoder.transform(dev_df['label'])\n",
    "test_df['label'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "metric = evaluate.load('rouge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Given the following topic, generate a good {} argument.'\n",
    "\n",
    "\"\"\"    \n",
    "need to be careful cause we cant give the same prefix to every type of \n",
    "arguments otherwise the model generalises and considers counter arguments as \n",
    "supporting arguments if we give the prefix : 'generate a supporting argument'   \n",
    "\"\"\"\n",
    "def preprocess_function(sample):\n",
    "    def process_by_type(label_type):\n",
    "        if label_type==2:\n",
    "            label = 'supporting'\n",
    "        elif label_type == 1:\n",
    "            label = 'neutral'\n",
    "        else:\n",
    "            label = 'counter'\n",
    "            \n",
    "        label_indices = [i for i, label in enumerate(sample['label']) if label == label_type]        \n",
    "        prefix = f\"Given the following topic, generate a good {label} argument. Topic=\"\n",
    "        labeled_samples = {key: [sample[key][i] for i in label_indices] for key in sample.keys()}\n",
    "        inputs = [prefix + doc for doc in labeled_samples['topic']]\n",
    "\n",
    "        model_inputs = tokenizer(inputs, max_length=4096, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(labeled_samples['argument'], max_length=4096, truncation=True)\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "    \n",
    "    model_inputs_supporting=process_by_type(label_type=2) ## supporting\n",
    "    model_inputs_neutral=process_by_type(label_type=1) ## neutral\n",
    "    model_inputs_counter=process_by_type(label_type=0) ## counter\n",
    "    \n",
    "    combined_model_inputs = {\n",
    "    'input_ids': model_inputs_supporting['input_ids'] + model_inputs_neutral['input_ids'] + model_inputs_counter['input_ids'],\n",
    "    'attention_mask': model_inputs_supporting['attention_mask'] + model_inputs_neutral['attention_mask'] + model_inputs_counter['attention_mask'],\n",
    "    'labels': model_inputs_supporting['labels'] + model_inputs_neutral['labels'] + model_inputs_counter['labels']\n",
    "    }\n",
    "    \n",
    "    return combined_model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68e9eb00734f32af25efadc279a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838e4af06bb14c46b0bc4b52aaad769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4689b150c3d4a02b47a94f90c4b2451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['type', 'id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "training_arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir='results/',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # for cuda\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca-mouchel\u001b[0m (\u001b[33mlia_epfl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/lucaM/LogicalFallacyRepo/logical-fallacy-LLMs/wandb/run-20231111_152152-dcdmj047</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lia_epfl/huggingface/runs/dcdmj047' target=\"_blank\">electric-fire-36</a></strong> to <a href='https://wandb.ai/lia_epfl/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lia_epfl/huggingface' target=\"_blank\">https://wandb.ai/lia_epfl/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lia_epfl/huggingface/runs/dcdmj047' target=\"_blank\">https://wandb.ai/lia_epfl/huggingface/runs/dcdmj047</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1765' max='1765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1765/1765 20:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.402500</td>\n",
       "      <td>3.280517</td>\n",
       "      <td>16.678400</td>\n",
       "      <td>1.980300</td>\n",
       "      <td>13.407000</td>\n",
       "      <td>13.403700</td>\n",
       "      <td>18.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.378100</td>\n",
       "      <td>3.253387</td>\n",
       "      <td>16.576600</td>\n",
       "      <td>2.022000</td>\n",
       "      <td>13.417200</td>\n",
       "      <td>13.428700</td>\n",
       "      <td>18.500690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.379300</td>\n",
       "      <td>3.241189</td>\n",
       "      <td>17.009800</td>\n",
       "      <td>1.944100</td>\n",
       "      <td>13.561700</td>\n",
       "      <td>13.580400</td>\n",
       "      <td>18.008276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.338600</td>\n",
       "      <td>3.235717</td>\n",
       "      <td>17.024100</td>\n",
       "      <td>2.083700</td>\n",
       "      <td>13.628100</td>\n",
       "      <td>13.641200</td>\n",
       "      <td>17.957241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.354700</td>\n",
       "      <td>3.233945</td>\n",
       "      <td>16.993000</td>\n",
       "      <td>2.077500</td>\n",
       "      <td>13.642100</td>\n",
       "      <td>13.651900</td>\n",
       "      <td>18.019310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1765, training_loss=3.3971486596818012, metrics={'train_runtime': 1254.1659, 'train_samples_per_second': 22.501, 'train_steps_per_second': 1.407, 'total_flos': 1117031829848064.0, 'train_loss': 3.3971486596818012, 'epoch': 5.0})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = f\"models/{model_name.split('/')[-1]}/w_neutral/\"\n",
    "trainer.save_model(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 3.1691298484802246,\n",
       " 'test_rouge1': 16.7889,\n",
       " 'test_rouge2': 1.6385,\n",
       " 'test_rougeL': 13.7659,\n",
       " 'test_rougeLsum': 13.772,\n",
       " 'test_gen_len': 18.6602809706258,\n",
       " 'test_runtime': 32.8572,\n",
       " 'test_samples_per_second': 23.83,\n",
       " 'test_steps_per_second': 1.491}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(saved_model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arg(topic, arg_type='supporting'):\n",
    "    prefix = f\"Given the following topic, generate a good {arg_type} argument. Topic=\"\n",
    "    inputs = tokenizer(prefix + topic, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs, min_length=25, max_length=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shouldn't alcohol be forbidden\n"
     ]
    }
   ],
   "source": [
    "df_iam = pd.read_csv(os.path.join(data_dir, 'train_iam.tsv'), sep='\\t')\n",
    "df_cckg = pd.read_csv(os.path.join(data_dir, 'train_cckg.tsv'), sep='\\t')\n",
    "topic = df_iam['topic'].sample(1).values[0]\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULDN'T ALCOHOL BE FORBIDDEN\n"
     ]
    }
   ],
   "source": [
    "print(topic.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supporting---Alcohol is a dangerous drug that can cause serious health problems. [ref]. [ref]. Alcohol is a dangerous drug that can cause serious health problems.\n",
      "counter---Alcohol is a dangerous drug, and it can cause serious health problems. [ref]. [ref]. Alcohol is a dangerous drug, and it can cause serious health problems.\n",
      "neutral---The alcoholics are able to make a living by drinking alcohol, and they are able to make a living by consuming alcohol.\n"
     ]
    }
   ],
   "source": [
    "arg_types=['supporting', 'counter', 'neutral']\n",
    "\n",
    "for arg_type in arg_types:\n",
    "    print(f\"\"\"{arg_type}---{generate_arg(\"Should alcohol be forbidden\", arg_type=arg_type)}\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logicalfallacies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
